# Ensemble Learning #

## Introduction ##

Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert’s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre‐ dictors is called an ensemble; thus, this technique is called Ensemble Learning, and an Ensemble Learning algorithm is called an Ensemble method.
For example, you can train a group of Decision Tree classifiers, each on a different random subset of the training set. To make predictions, you just obtain the predic‐ tions of all individual trees, then predict the class that gets the most votes . Such an ensemble of Decision Trees is called a Random Forest, and despite its simplicity, this is one of the most powerful Machine Learning algo‐ rithms available today.


## Most popular Ensemble method ##

 bagging, boosting, stacking, and a few others. We will also explore Random Forests.
 
![toptal-blog-image-1551787771826-8fcca2d6e3cddefc5b8a8069170a6ebb](https://user-images.githubusercontent.com/98185045/167075805-2ca49d27-1b78-40ea-9996-0a4d226a62b0.png)


## Reference ##

https://www.toptal.com/machine-learning/ensemble-methods-machine-learning
